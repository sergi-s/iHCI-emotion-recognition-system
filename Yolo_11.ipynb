{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nz5I6FimfBw2",
        "outputId": "da480226-fe98-4d97-ac37-d38780070353"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting roboflow\n",
            "  Downloading roboflow-1.2.11-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: certifi in ./myenv/lib/python3.9/site-packages (from roboflow) (2025.11.12)\n",
            "Collecting idna==3.7 (from roboflow)\n",
            "  Using cached idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: cycler in ./myenv/lib/python3.9/site-packages (from roboflow) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in ./myenv/lib/python3.9/site-packages (from roboflow) (1.4.7)\n",
            "Requirement already satisfied: matplotlib in ./myenv/lib/python3.9/site-packages (from roboflow) (3.9.4)\n",
            "Requirement already satisfied: numpy>=1.18.5 in ./myenv/lib/python3.9/site-packages (from roboflow) (2.0.2)\n",
            "Collecting opencv-python-headless==4.10.0.84 (from roboflow)\n",
            "  Downloading opencv_python_headless-4.10.0.84-cp37-abi3-macosx_11_0_arm64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in ./myenv/lib/python3.9/site-packages (from roboflow) (11.3.0)\n",
            "Collecting pi-heif<2 (from roboflow)\n",
            "  Downloading pi_heif-1.1.1-cp39-cp39-macosx_11_0_arm64.whl.metadata (6.5 kB)\n",
            "Collecting pillow-avif-plugin<2 (from roboflow)\n",
            "  Downloading pillow_avif_plugin-1.5.2-cp39-cp39-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: python-dateutil in ./myenv/lib/python3.9/site-packages (from roboflow) (2.9.0.post0)\n",
            "Collecting python-dotenv (from roboflow)\n",
            "  Using cached python_dotenv-1.2.1-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: requests in ./myenv/lib/python3.9/site-packages (from roboflow) (2.32.5)\n",
            "Requirement already satisfied: six in ./myenv/lib/python3.9/site-packages (from roboflow) (1.17.0)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in ./myenv/lib/python3.9/site-packages (from roboflow) (2.5.0)\n",
            "Collecting tqdm>=4.41.0 (from roboflow)\n",
            "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in ./myenv/lib/python3.9/site-packages (from roboflow) (6.0.3)\n",
            "Collecting requests-toolbelt (from roboflow)\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Collecting filetype (from roboflow)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in ./myenv/lib/python3.9/site-packages (from matplotlib->roboflow) (1.3.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in ./myenv/lib/python3.9/site-packages (from matplotlib->roboflow) (4.60.1)\n",
            "Requirement already satisfied: packaging>=20.0 in ./myenv/lib/python3.9/site-packages (from matplotlib->roboflow) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in ./myenv/lib/python3.9/site-packages (from matplotlib->roboflow) (3.2.5)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in ./myenv/lib/python3.9/site-packages (from matplotlib->roboflow) (6.5.2)\n",
            "Requirement already satisfied: zipp>=3.1.0 in ./myenv/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib->roboflow) (3.23.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in ./myenv/lib/python3.9/site-packages (from requests->roboflow) (3.4.4)\n",
            "Downloading roboflow-1.2.11-py3-none-any.whl (89 kB)\n",
            "Using cached idna-3.7-py3-none-any.whl (66 kB)\n",
            "Downloading opencv_python_headless-4.10.0.84-cp37-abi3-macosx_11_0_arm64.whl (54.8 MB)\n",
            "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m54.8/54.8 MB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading pi_heif-1.1.1-cp39-cp39-macosx_11_0_arm64.whl (892 kB)\n",
            "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m892.3/892.3 kB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow_avif_plugin-1.5.2-cp39-cp39-macosx_11_0_arm64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Using cached python_dotenv-1.2.1-py3-none-any.whl (21 kB)\n",
            "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "Installing collected packages: pillow-avif-plugin, filetype, tqdm, python-dotenv, pi-heif, opencv-python-headless, idna, requests-toolbelt, roboflow\n",
            "\u001b[2K  Attempting uninstall: idna笏≫煤\u001b[0m\u001b[90m笊ｺ\u001b[0m\u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m5/9\u001b[0m [opencv-python-headless]\n",
            "\u001b[2K    Found existing installation: idna 3.1190m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m5/9\u001b[0m [opencv-python-headless]\n",
            "\u001b[2K    Uninstalling idna-3.11:[0m\u001b[90m笊ｺ\u001b[0m\u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m5/9\u001b[0m [opencv-python-headless]\n",
            "\u001b[2K      Successfully uninstalled idna-3.11\u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m5/9\u001b[0m [opencv-python-headless]\n",
            "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m9/9\u001b[0m [roboflow]8/9\u001b[0m [roboflow]-headless]\n",
            "\u001b[1A\u001b[2KSuccessfully installed filetype-1.2.0 idna-3.7 opencv-python-headless-4.10.0.84 pi-heif-1.1.1 pillow-avif-plugin-1.5.2 python-dotenv-1.2.1 requests-toolbelt-1.0.0 roboflow-1.2.11 tqdm-4.67.1\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading Dataset Version Zip in Emotion-Detection-1 to yolov11:: 100%|笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎| 48036/48036 [00:01<00:00, 39558.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to Emotion-Detection-1 in yolov11:: 100%|笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎| 3406/3406 [00:00<00:00, 9037.34it/s]\n"
          ]
        }
      ],
      "source": [
        "!pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"wAFpXIOS5YhwXaos60c3\")\n",
        "project = rf.workspace(\"computer-vision-projects-zhogq\").project(\"emotion-detection-y0svj\")\n",
        "version = project.version(1)\n",
        "dataset = version.download(\"yolov11\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmiEVFOCfEsA",
        "outputId": "2226a915-a137-4869-c921-953150d52a76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ultralytics in ./myenv/lib/python3.9/site-packages (8.3.230)\n",
            "Requirement already satisfied: numpy>=1.23.0 in ./myenv/lib/python3.9/site-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in ./myenv/lib/python3.9/site-packages (from ultralytics) (3.9.4)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in ./myenv/lib/python3.9/site-packages (from ultralytics) (4.12.0.88)\n",
            "Requirement already satisfied: pillow>=7.1.2 in ./myenv/lib/python3.9/site-packages (from ultralytics) (11.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in ./myenv/lib/python3.9/site-packages (from ultralytics) (6.0.3)\n",
            "Requirement already satisfied: requests>=2.23.0 in ./myenv/lib/python3.9/site-packages (from ultralytics) (2.32.5)\n",
            "Requirement already satisfied: scipy>=1.4.1 in ./myenv/lib/python3.9/site-packages (from ultralytics) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in ./myenv/lib/python3.9/site-packages (from ultralytics) (2.8.0)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in ./myenv/lib/python3.9/site-packages (from ultralytics) (0.23.0)\n",
            "Requirement already satisfied: psutil>=5.8.0 in ./myenv/lib/python3.9/site-packages (from ultralytics) (7.1.3)\n",
            "Requirement already satisfied: polars>=0.20.0 in ./myenv/lib/python3.9/site-packages (from ultralytics) (1.35.2)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.18 in ./myenv/lib/python3.9/site-packages (from ultralytics) (2.0.18)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in ./myenv/lib/python3.9/site-packages (from matplotlib>=3.3.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in ./myenv/lib/python3.9/site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in ./myenv/lib/python3.9/site-packages (from matplotlib>=3.3.0->ultralytics) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in ./myenv/lib/python3.9/site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in ./myenv/lib/python3.9/site-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in ./myenv/lib/python3.9/site-packages (from matplotlib>=3.3.0->ultralytics) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in ./myenv/lib/python3.9/site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in ./myenv/lib/python3.9/site-packages (from matplotlib>=3.3.0->ultralytics) (6.5.2)\n",
            "Requirement already satisfied: zipp>=3.1.0 in ./myenv/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib>=3.3.0->ultralytics) (3.23.0)\n",
            "Requirement already satisfied: polars-runtime-32==1.35.2 in ./myenv/lib/python3.9/site-packages (from polars>=0.20.0->ultralytics) (1.35.2)\n",
            "Requirement already satisfied: six>=1.5 in ./myenv/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in ./myenv/lib/python3.9/site-packages (from requests>=2.23.0->ultralytics) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./myenv/lib/python3.9/site-packages (from requests>=2.23.0->ultralytics) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./myenv/lib/python3.9/site-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./myenv/lib/python3.9/site-packages (from requests>=2.23.0->ultralytics) (2025.11.12)\n",
            "Requirement already satisfied: filelock in ./myenv/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in ./myenv/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics) (4.15.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in ./myenv/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics) (1.14.0)\n",
            "Requirement already satisfied: networkx in ./myenv/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in ./myenv/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in ./myenv/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics) (2025.10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./myenv/lib/python3.9/site-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in ./myenv/lib/python3.9/site-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install ultralytics\n",
        "from ultralytics import YOLO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "g3zA4_c5fMCE",
        "outputId": "cbc485c2-15b4-40dd-b350-c1cc2b80c75e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/Emotion-Detection-1'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset.location"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7msKgcXfNW1",
        "outputId": "c478beb8-5c98-44f1-aef6-1d3937101b24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt'...\n",
            "100% 5.35M/5.35M [00:00<00:00, 261MB/s]\n",
            "Ultralytics 8.3.23 泅 Python-3.10.12 torch-2.5.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolo11n.pt, data=/content/Emotion-Detection-1/data.yaml, epochs=70, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n",
            "100% 755k/755k [00:00<00:00, 18.3MB/s]\n",
            "Overriding model.yaml nc=80 with nc=5\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
            "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
            "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
            " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
            " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
            " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
            " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
            " 23        [16, 19, 22]  1    431647  ultralytics.nn.modules.head.Detect           [5, [64, 128, 256]]           \n",
            "YOLO11n summary: 319 layers, 2,590,815 parameters, 2,590,799 gradients, 6.4 GFLOPs\n",
            "\n",
            "Transferred 448/499 items from pretrained weights\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train', view at http://localhost:6006/\n",
            "Freezing layer 'model.23.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed 笨\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/Emotion-Detection-1/train/labels... 1473 images, 252 backgrounds, 0 corrupt: 100% 1473/1473 [00:00<00:00, 1903.17it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/Emotion-Detection-1/train/labels.cache\n",
            "/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 1.4.20 (you have 1.4.15). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/Emotion-Detection-1/valid/labels... 138 images, 19 backgrounds, 0 corrupt: 100% 138/138 [00:00<00:00, 651.48it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/Emotion-Detection-1/valid/labels.cache\n",
            "Plotting labels to runs/detect/train/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001111, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added 笨\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/train\u001b[0m\n",
            "Starting training for 70 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       1/70      2.58G      1.441      3.548      1.723          1        640: 100% 93/93 [00:34<00:00,  2.72it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:04<00:00,  1.24it/s]\n",
            "                   all        138        129      0.339     0.0827      0.195      0.106\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       2/70      2.43G      1.353      2.897      1.557          0        640: 100% 93/93 [00:30<00:00,  3.06it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:02<00:00,  2.32it/s]\n",
            "                   all        138        129      0.182      0.359      0.195     0.0974\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       3/70      2.38G      1.384      2.541      1.564          2        640: 100% 93/93 [00:28<00:00,  3.27it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:01<00:00,  4.11it/s]\n",
            "                   all        138        129      0.179      0.307       0.17     0.0844\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       4/70      2.43G      1.378      2.279      1.544          3        640: 100% 93/93 [00:32<00:00,  2.90it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:01<00:00,  4.27it/s]\n",
            "                   all        138        129      0.274      0.543      0.301      0.145\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       5/70      2.37G      1.322      2.111      1.528          4        640: 100% 93/93 [00:29<00:00,  3.10it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:01<00:00,  4.27it/s]\n",
            "                   all        138        129       0.26      0.594      0.361      0.209\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       6/70      2.38G      1.309      1.981      1.503          2        640: 100% 93/93 [00:30<00:00,  3.07it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:01<00:00,  4.23it/s]\n",
            "                   all        138        129      0.248      0.589      0.299      0.172\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       7/70      2.38G      1.288      1.884      1.473          4        640: 100% 93/93 [00:31<00:00,  2.96it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:02<00:00,  1.95it/s]\n",
            "                   all        138        129      0.268      0.609       0.35      0.202\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       8/70      2.38G      1.266      1.867      1.451          0        640: 100% 93/93 [00:30<00:00,  3.03it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:01<00:00,  4.73it/s]\n",
            "                   all        138        129      0.267      0.531      0.393      0.234\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       9/70      2.37G      1.248      1.763      1.444          0        640: 100% 93/93 [00:29<00:00,  3.18it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:01<00:00,  2.54it/s]\n",
            "                   all        138        129        0.3      0.582      0.385      0.245\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      10/70      2.38G      1.213       1.68      1.434          2        640: 100% 93/93 [00:27<00:00,  3.39it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:02<00:00,  2.44it/s]\n",
            "                   all        138        129      0.321      0.575      0.401       0.24\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      11/70      2.38G      1.184      1.632      1.405          2        640: 100% 93/93 [00:27<00:00,  3.35it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:01<00:00,  3.55it/s]\n",
            "                   all        138        129      0.277      0.545      0.346      0.225\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      12/70      2.38G      1.197      1.623      1.398          3        640: 100% 93/93 [00:30<00:00,  3.08it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:00<00:00,  5.07it/s]\n",
            "                   all        138        129      0.356      0.552      0.402      0.242\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      13/70      2.37G      1.226      1.618      1.418          2        640: 100% 93/93 [00:34<00:00,  2.73it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:01<00:00,  4.19it/s]\n",
            "                   all        138        129      0.453      0.424      0.444      0.279\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      14/70      2.38G      1.191      1.534      1.406          1        640: 100% 93/93 [00:30<00:00,  3.06it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:01<00:00,  4.83it/s]\n",
            "                   all        138        129      0.349        0.6      0.447      0.288\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      15/70      2.38G      1.138      1.516      1.372          0        640: 100% 93/93 [00:30<00:00,  3.05it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:01<00:00,  4.42it/s]\n",
            "                   all        138        129      0.364      0.549      0.442       0.28\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      16/70      2.38G      1.118      1.488      1.352          0        640: 100% 93/93 [00:29<00:00,  3.15it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:01<00:00,  4.33it/s]\n",
            "                   all        138        129      0.398      0.472      0.418      0.243\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      17/70      2.37G      1.162       1.47       1.38          1        640: 100% 93/93 [00:28<00:00,  3.30it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:02<00:00,  2.39it/s]\n",
            "                   all        138        129      0.349      0.629      0.457      0.277\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      18/70      2.43G      1.096      1.413      1.348          2        640: 100% 93/93 [00:27<00:00,  3.35it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:02<00:00,  2.21it/s]\n",
            "                   all        138        129      0.366      0.599      0.487      0.302\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      19/70      2.43G      1.126      1.432      1.356          1        640: 100% 93/93 [00:29<00:00,  3.11it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:01<00:00,  4.17it/s]\n",
            "                   all        138        129      0.451      0.567      0.489      0.296\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      20/70      2.38G      1.108      1.384       1.34          3        640: 100% 93/93 [00:30<00:00,  3.10it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:01<00:00,  4.58it/s]\n",
            "                   all        138        129      0.393      0.633      0.467      0.282\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      21/70      2.43G      1.091      1.371      1.332          2        640: 100% 93/93 [00:29<00:00,  3.12it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:00<00:00,  5.05it/s]\n",
            "                   all        138        129      0.413      0.544      0.502      0.319\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      22/70      2.38G      1.088      1.362      1.337          2        640: 100% 93/93 [00:29<00:00,  3.16it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:02<00:00,  2.17it/s]\n",
            "                   all        138        129      0.504      0.483      0.494       0.32\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      23/70      2.38G       1.06       1.31      1.315          4        640: 100% 93/93 [00:27<00:00,  3.37it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:01<00:00,  2.64it/s]\n",
            "                   all        138        129       0.47      0.511      0.482      0.313\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      24/70      2.38G      1.057      1.296      1.289          1        640: 100% 93/93 [00:31<00:00,  2.97it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:01<00:00,  3.77it/s]\n",
            "                   all        138        129       0.32      0.619      0.436      0.274\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      25/70      2.42G      1.029      1.336      1.301          0        640: 100% 93/93 [00:29<00:00,  3.18it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:01<00:00,  4.39it/s]\n",
            "                   all        138        129      0.384      0.498       0.43      0.285\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      26/70      2.45G      1.032      1.253      1.285          1        640: 100% 93/93 [00:30<00:00,  3.10it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:01<00:00,  4.30it/s]\n",
            "                   all        138        129      0.435      0.552      0.515       0.32\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      27/70      2.44G      1.028      1.268      1.283          4        640: 100% 93/93 [00:29<00:00,  3.15it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:01<00:00,  3.95it/s]\n",
            "                   all        138        129      0.355      0.583       0.46      0.281\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      28/70      2.43G      1.009      1.211      1.278          2        640: 100% 93/93 [00:29<00:00,  3.17it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:01<00:00,  4.19it/s]\n",
            "                   all        138        129      0.422      0.559       0.51      0.333\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      29/70      2.37G      1.022      1.215      1.288          1        640: 100% 93/93 [00:28<00:00,  3.29it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:02<00:00,  2.14it/s]\n",
            "                   all        138        129      0.451      0.484      0.492      0.305\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      30/70      2.38G      1.001      1.199      1.286          2        640: 100% 93/93 [00:31<00:00,  2.95it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:02<00:00,  2.03it/s]\n",
            "                   all        138        129      0.444      0.548      0.505       0.31\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      31/70      2.43G     0.9712      1.188       1.26          2        640: 100% 93/93 [00:27<00:00,  3.40it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:01<00:00,  2.83it/s]\n",
            "                   all        138        129      0.469      0.559      0.498      0.324\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      32/70      2.38G     0.9638       1.18      1.261          3        640: 100% 93/93 [00:28<00:00,  3.23it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:01<00:00,  4.55it/s]\n",
            "                   all        138        129      0.436      0.614      0.515      0.331\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      33/70      2.39G      0.961      1.114      1.239          3        640: 100% 93/93 [00:29<00:00,  3.13it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:01<00:00,  4.79it/s]\n",
            "                   all        138        129      0.482      0.606      0.501      0.316\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      34/70      2.38G     0.9505      1.113      1.236          1        640: 100% 93/93 [00:28<00:00,  3.23it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:01<00:00,  4.40it/s]\n",
            "                   all        138        129      0.514      0.563      0.538      0.327\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      35/70      2.43G     0.9477      1.079      1.253          1        640: 100% 93/93 [00:27<00:00,  3.34it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:02<00:00,  2.05it/s]\n",
            "                   all        138        129      0.468      0.538      0.483      0.303\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      36/70      2.43G     0.9489       1.11      1.251          1        640: 100% 93/93 [00:27<00:00,  3.34it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:01<00:00,  3.68it/s]\n",
            "                   all        138        129       0.41      0.586      0.529      0.355\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      37/70       2.4G     0.9377      1.061      1.244          2        640: 100% 93/93 [00:28<00:00,  3.21it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:01<00:00,  4.43it/s]\n",
            "                   all        138        129       0.56      0.441      0.517      0.317\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      38/70       2.4G     0.9267      1.023      1.225          2        640: 100% 93/93 [00:28<00:00,  3.21it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:01<00:00,  4.07it/s]\n",
            "                   all        138        129       0.48      0.502      0.501      0.315\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      39/70      2.38G     0.9045      1.044      1.209          1        640: 100% 93/93 [00:29<00:00,  3.21it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:01<00:00,  4.54it/s]\n",
            "                   all        138        129        0.5      0.556      0.485      0.312\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      40/70      2.38G      0.895      1.011      1.223          2        640: 100% 93/93 [00:27<00:00,  3.36it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:02<00:00,  2.28it/s]\n",
            "                   all        138        129      0.456      0.572      0.474      0.297\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      41/70      2.37G     0.8702      1.006      1.193          0        640: 100% 93/93 [00:31<00:00,  3.00it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:02<00:00,  2.36it/s]\n",
            "                   all        138        129      0.447      0.619      0.511      0.323\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      42/70      2.41G     0.9148      1.013      1.218          1        640: 100% 93/93 [00:27<00:00,  3.44it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:01<00:00,  4.82it/s]\n",
            "                   all        138        129      0.543      0.505       0.55      0.339\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      43/70      2.38G      0.861     0.9712      1.185          0        640: 100% 93/93 [00:29<00:00,  3.17it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:01<00:00,  4.46it/s]\n",
            "                   all        138        129       0.46      0.619      0.528      0.336\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      44/70      2.38G     0.8619     0.9611      1.179          2        640: 100% 93/93 [00:29<00:00,  3.19it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:00<00:00,  5.28it/s]\n",
            "                   all        138        129      0.426       0.68      0.501       0.33\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      45/70      2.42G      0.852     0.9057      1.185          3        640: 100% 93/93 [00:28<00:00,  3.27it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:02<00:00,  2.26it/s]\n",
            "                   all        138        129      0.446      0.497      0.498      0.315\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      46/70      2.38G     0.8652     0.9106      1.205          1        640: 100% 93/93 [00:26<00:00,  3.45it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:01<00:00,  2.96it/s]\n",
            "                   all        138        129      0.484      0.611      0.542      0.333\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      47/70      2.38G     0.8371     0.8739      1.178          1        640: 100% 93/93 [00:30<00:00,  3.04it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:01<00:00,  4.32it/s]\n",
            "                   all        138        129      0.476      0.628      0.512      0.318\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      48/70      2.38G     0.8221     0.8617      1.162          2        640: 100% 93/93 [00:29<00:00,  3.15it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:01<00:00,  4.78it/s]\n",
            "                   all        138        129      0.453      0.642       0.51      0.318\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      49/70      2.37G     0.8204      0.861      1.164          4        640: 100% 93/93 [00:30<00:00,  3.08it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:01<00:00,  4.86it/s]\n",
            "                   all        138        129      0.506      0.532      0.531      0.336\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      50/70      2.38G     0.8236     0.8518       1.16          2        640: 100% 93/93 [00:27<00:00,  3.35it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:02<00:00,  2.22it/s]\n",
            "                   all        138        129      0.568      0.598      0.543      0.344\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      51/70      2.38G     0.8066     0.8329      1.153          1        640: 100% 93/93 [00:27<00:00,  3.44it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:01<00:00,  4.29it/s]\n",
            "                   all        138        129      0.444      0.621      0.508      0.324\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      52/70      2.44G     0.7851     0.8046      1.147          1        640: 100% 93/93 [00:28<00:00,  3.27it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:01<00:00,  4.98it/s]\n",
            "                   all        138        129      0.603       0.56      0.537      0.326\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      53/70      2.37G     0.7928     0.7952      1.155          1        640: 100% 93/93 [00:32<00:00,  2.83it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:01<00:00,  4.29it/s]\n",
            "                   all        138        129      0.439      0.668      0.499      0.302\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      54/70      2.38G     0.7748     0.8114      1.149          0        640: 100% 93/93 [00:29<00:00,  3.11it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:01<00:00,  4.21it/s]\n",
            "                   all        138        129      0.543      0.506      0.485      0.297\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      55/70      2.38G       0.76     0.7693      1.137          2        640: 100% 93/93 [00:29<00:00,  3.19it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:01<00:00,  4.09it/s]\n",
            "                   all        138        129      0.475      0.619      0.523      0.319\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      56/70      2.43G     0.7677     0.7971      1.133          3        640: 100% 93/93 [00:29<00:00,  3.18it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:01<00:00,  4.49it/s]\n",
            "                   all        138        129      0.553      0.572      0.544      0.337\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      57/70      2.42G     0.7467     0.7665      1.114          0        640: 100% 93/93 [00:27<00:00,  3.40it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:02<00:00,  2.11it/s]\n",
            "                   all        138        129      0.553      0.605      0.543      0.343\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      58/70      2.38G     0.7471     0.7164      1.125          3        640: 100% 93/93 [00:26<00:00,  3.55it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:01<00:00,  4.02it/s]\n",
            "                   all        138        129      0.483      0.637      0.531      0.337\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      59/70      2.43G     0.7443     0.7392      1.121          2        640: 100% 93/93 [00:31<00:00,  2.99it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:01<00:00,  3.65it/s]\n",
            "                   all        138        129       0.57      0.539      0.528      0.326\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      60/70      2.38G     0.7274     0.7202      1.122          1        640: 100% 93/93 [00:28<00:00,  3.28it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:00<00:00,  5.40it/s]\n",
            "                   all        138        129      0.544      0.536      0.501      0.314\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      61/70      2.55G     0.6577     0.5302      1.102          1        640: 100% 93/93 [00:30<00:00,  3.05it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:00<00:00,  5.06it/s]\n",
            "                   all        138        129      0.497      0.592      0.503      0.319\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      62/70       2.4G     0.6215     0.4862      1.068          1        640: 100% 93/93 [00:28<00:00,  3.27it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:01<00:00,  4.85it/s]\n",
            "                   all        138        129      0.488      0.619      0.524      0.328\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      63/70      2.38G     0.6304     0.4698      1.081          1        640: 100% 93/93 [00:25<00:00,  3.65it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:01<00:00,  2.72it/s]\n",
            "                   all        138        129       0.52      0.591      0.544      0.346\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      64/70      2.38G     0.6205     0.4637      1.073          1        640: 100% 93/93 [00:27<00:00,  3.34it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:01<00:00,  4.71it/s]\n",
            "                   all        138        129      0.452      0.635      0.508      0.327\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      65/70      2.37G     0.6003     0.4468      1.072          1        640: 100% 93/93 [00:31<00:00,  2.97it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:01<00:00,  4.66it/s]\n",
            "                   all        138        129      0.497      0.564      0.512      0.337\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      66/70      2.38G     0.5899     0.4443      1.038          1        640: 100% 93/93 [00:25<00:00,  3.60it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:01<00:00,  3.21it/s]\n",
            "                   all        138        129      0.485      0.614      0.512      0.328\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      67/70      2.38G     0.5755     0.4195      1.037          1        640: 100% 93/93 [00:27<00:00,  3.40it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:01<00:00,  4.69it/s]\n",
            "                   all        138        129      0.553      0.544      0.516      0.332\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      68/70      2.38G     0.5576     0.4107       1.02          0        640: 100% 93/93 [00:28<00:00,  3.28it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:01<00:00,  4.60it/s]\n",
            "                   all        138        129      0.468      0.629      0.503      0.325\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      69/70      2.37G     0.5954     0.4243      1.058          1        640: 100% 93/93 [00:25<00:00,  3.62it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:01<00:00,  2.79it/s]\n",
            "                   all        138        129      0.443      0.646      0.502      0.321\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      70/70       2.4G     0.5644     0.3978      1.028          1        640: 100% 93/93 [00:26<00:00,  3.45it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:01<00:00,  4.87it/s]\n",
            "                   all        138        129      0.466      0.591      0.496       0.31\n",
            "\n",
            "70 epochs completed in 0.614 hours.\n",
            "Optimizer stripped from runs/detect/train/weights/last.pt, 5.5MB\n",
            "Optimizer stripped from runs/detect/train/weights/best.pt, 5.5MB\n",
            "\n",
            "Validating runs/detect/train/weights/best.pt...\n",
            "Ultralytics 8.3.23 泅 Python-3.10.12 torch-2.5.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "YOLO11n summary (fused): 238 layers, 2,583,127 parameters, 0 gradients, 6.3 GFLOPs\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 5/5 [00:02<00:00,  1.70it/s]\n",
            "                   all        138        129      0.409      0.587      0.529      0.357\n",
            "                 Angry         25         25      0.429       0.76      0.631       0.47\n",
            "               Fearful         22         22      0.289      0.545      0.448       0.33\n",
            "                 Happy         34         41      0.656      0.559      0.654      0.386\n",
            "               Neutral         18         18      0.327      0.244      0.284      0.157\n",
            "                   Sad         23         23      0.341      0.826       0.63       0.44\n",
            "Speed: 0.5ms preprocess, 3.9ms inference, 0.0ms loss, 4.9ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/train\u001b[0m\n",
            "汳｡ Learn more at https://docs.ultralytics.com/modes/train\n"
          ]
        }
      ],
      "source": [
        "!yolo task=detect mode=train epochs=70 batch=16 plots=True model='yolo11n.pt' data={dataset.location}/data.yaml imgsz=640"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5LJRki5fm_s",
        "outputId": "25b256a4-f865-43ab-e43a-f55dd6a26867"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Folder moved successfully!\n"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "\n",
        "\n",
        "source_folder = './runs'\n",
        "destination_folder = '/content/drive/MyDrive/Face Emotion Detection'\n",
        "\n",
        "# Move the folder\n",
        "shutil.move(source_folder, destination_folder)\n",
        "\n",
        "print(\"Folder moved successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "XRMcfm0Ir572"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('/content/drive/MyDrive/Face Emotion Detection')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uE0rlhqHsG4w",
        "outputId": "bb75cc2d-7c41-4f3b-cfd1-a0f565b285da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mruns\u001b[0m/\n"
          ]
        }
      ],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 851
        },
        "id": "IOeq4UMPf0Ot",
        "outputId": "03e5aaee-92ae-4cdd-8ca7-45183876e6b0"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'runs/detect/train/weights/best.pt'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01multralytics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m YOLO\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mYOLO\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./runs/detect/train/weights/best.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mexport(\u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monnx\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# This creates 'best.onnx'\u001b[39;00m\n",
            "File \u001b[0;32m~/Public/Yolo11-Face-Emotion-Detection/myenv/lib/python3.9/site-packages/ultralytics/models/yolo/model.py:81\u001b[0m, in \u001b[0;36mYOLO.__init__\u001b[0;34m(self, model, task, verbose)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m \u001b[38;5;241m=\u001b[39m new_instance\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;66;03m# Continue with default YOLO initialization\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRTDETR\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmodel[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39m_get_name():  \u001b[38;5;66;03m# if RTDETR head\u001b[39;00m\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01multralytics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RTDETR\n",
            "File \u001b[0;32m~/Public/Yolo11-Face-Emotion-Detection/myenv/lib/python3.9/site-packages/ultralytics/engine/model.py:149\u001b[0m, in \u001b[0;36mModel.__init__\u001b[0;34m(self, model, task, verbose)\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(model, task\u001b[38;5;241m=\u001b[39mtask, verbose\u001b[38;5;241m=\u001b[39mverbose)\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 149\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m# Delete super().training for accessing self.model.training\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining\n",
            "File \u001b[0;32m~/Public/Yolo11-Face-Emotion-Detection/myenv/lib/python3.9/site-packages/ultralytics/engine/model.py:288\u001b[0m, in \u001b[0;36mModel._load\u001b[0;34m(self, weights, task)\u001b[0m\n\u001b[1;32m    285\u001b[0m weights \u001b[38;5;241m=\u001b[39m checks\u001b[38;5;241m.\u001b[39mcheck_model_file_from_stem(weights)  \u001b[38;5;66;03m# add suffix, i.e. yolo11n -> yolo11n.pt\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(weights)\u001b[38;5;241m.\u001b[39mrpartition(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mckpt \u001b[38;5;241m=\u001b[39m \u001b[43mload_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtask\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moverrides \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset_ckpt_args(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39margs)\n",
            "File \u001b[0;32m~/Public/Yolo11-Face-Emotion-Detection/myenv/lib/python3.9/site-packages/ultralytics/nn/tasks.py:1461\u001b[0m, in \u001b[0;36mload_checkpoint\u001b[0;34m(weight, device, inplace, fuse)\u001b[0m\n\u001b[1;32m   1448\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_checkpoint\u001b[39m(weight, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, fuse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1449\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a single model weights.\u001b[39;00m\n\u001b[1;32m   1450\u001b[0m \n\u001b[1;32m   1451\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1459\u001b[0m \u001b[38;5;124;03m        ckpt (dict): Model checkpoint dictionary.\u001b[39;00m\n\u001b[1;32m   1460\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1461\u001b[0m     ckpt, weight \u001b[38;5;241m=\u001b[39m \u001b[43mtorch_safe_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# load ckpt\u001b[39;00m\n\u001b[1;32m   1462\u001b[0m     args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mDEFAULT_CFG_DICT, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(ckpt\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_args\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))}  \u001b[38;5;66;03m# combine model and default args, preferring model args\u001b[39;00m\n\u001b[1;32m   1463\u001b[0m     model \u001b[38;5;241m=\u001b[39m (ckpt\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mema\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m ckpt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mfloat()  \u001b[38;5;66;03m# FP32 model\u001b[39;00m\n",
            "File \u001b[0;32m~/Public/Yolo11-Face-Emotion-Detection/myenv/lib/python3.9/site-packages/ultralytics/nn/tasks.py:1409\u001b[0m, in \u001b[0;36mtorch_safe_load\u001b[0;34m(weight, safe_only)\u001b[0m\n\u001b[1;32m   1407\u001b[0m                 ckpt \u001b[38;5;241m=\u001b[39m torch_load(f, pickle_module\u001b[38;5;241m=\u001b[39msafe_pickle)\n\u001b[1;32m   1408\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1409\u001b[0m             ckpt \u001b[38;5;241m=\u001b[39m \u001b[43mtorch_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1411\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# e.name is missing module name\u001b[39;00m\n\u001b[1;32m   1412\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
            "File \u001b[0;32m~/Public/Yolo11-Face-Emotion-Detection/myenv/lib/python3.9/site-packages/ultralytics/utils/patches.py:116\u001b[0m, in \u001b[0;36mtorch_load\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TORCH_1_13 \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights_only\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m    114\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights_only\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Public/Yolo11-Face-Emotion-Detection/myenv/lib/python3.9/site-packages/torch/serialization.py:1484\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1481\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1482\u001b[0m     pickle_load_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1484\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m   1485\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1486\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1487\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1488\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1489\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
            "File \u001b[0;32m~/Public/Yolo11-Face-Emotion-Detection/myenv/lib/python3.9/site-packages/torch/serialization.py:759\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer: FileLike, mode: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _opener[IO[\u001b[38;5;28mbytes\u001b[39m]]:\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 759\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    760\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    761\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
            "File \u001b[0;32m~/Public/Yolo11-Face-Emotion-Detection/myenv/lib/python3.9/site-packages/torch/serialization.py:740\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    739\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike[\u001b[38;5;28mstr\u001b[39m]], mode: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 740\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'runs/detect/train/weights/best.pt'"
          ]
        }
      ],
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO(\"./runs/detect/train/weights/best.pt\")\n",
        "\n",
        "model.export(format=\"onnx\")  # This creates 'best.onnx'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2ajZ6jVf2hz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wAxibUjpgKcf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UaCdvCn2gKeD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
